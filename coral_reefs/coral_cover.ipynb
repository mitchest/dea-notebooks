{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First stab at GBR coral cover\n",
    "\n",
    "## conceptual workflow \n",
    "\n",
    "In the longer run the aim is to sample geo-median imagery and other covariate data (depth, slope, etc) around in situ sample data dates, and then predict time-series coral cover  \n",
    "\n",
    "For the moment, to get the hang of it, use a once-off longer time period mosaic and do categorical maps + coral cover. Something like:  \n",
    "    \n",
    "    1. Generate a once off nice composite, maybe a 2-3 year period (2017-2019? - gives access to Sentinel-2 Level 2?)  \n",
    "    2. Maybe segmentation - **optional to begin with**  \n",
    "    3. Sample from the whole trianing data set (*reef cloud*) [but for the moment sample the same time period and AOI as the mosaic]  \n",
    "    4. Fit a ML model (e.g. RF or SVM etc.)  \n",
    "    5. Explore and validate  \n",
    "    6. experiment with OBIA implementation for relationship/neighbourhood rules  \n",
    "\n",
    "**Cummulaive Q's:**\n",
    "- once settled on a larger area, should I export the temporal composites (and segmentation etc.)?\n",
    "- what are the costs for reprojection? Can you set to use the native/nominal projection of data?\n",
    "- RE the `query` dictionary passed to the loading function or loading helper: does the `output_crs` only apply to output after the processing subsquent to the load, or does it always apply to every time-step found in the collection?  \n",
    "- what happens when you stop execution while dask chunks are processing/queued (seems not to like that...)?  \n",
    "- for lazy eval, is it quicker to `.load()`/`.compute()` just the individual bands, several at one, or the whole lot?  And if several at once, how to do that via the slot method (i.e. `lazy_dat.blue.load()` --> `lazy_day.some_bands.load()`  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Front matter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import subprocess as sp\n",
    "import sys\n",
    "import datacube\n",
    "import rasterio\n",
    "\n",
    "import shapely\n",
    "\n",
    "import pydotplus\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from odc.ui import with_ui_cbk\n",
    "from odc.algo import to_f32, xr_geomedian, int_geomedian\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from joblib import dump, load\n",
    "\n",
    "from IPython.display import Image\n",
    "from datacube.utils import geometry\n",
    "from datacube.utils.cog import write_cog\n",
    "\n",
    "sys.path.append(\"../Scripts\")\n",
    "from dea_datahandling import load_ard\n",
    "from dea_plotting import rgb\n",
    "from dea_dask import create_local_dask_cluster\n",
    "from dea_classificationtools import collect_training_data\n",
    "from dea_classificationtools import predict_xr\n",
    "from dea_plotting import map_shapefile\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') ## why?\n",
    "\n",
    "dc = datacube.Datacube(app=\"coral_cover\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In situ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "coral_dat = gpd.read_file(\"gbr_reefcloud_coral.shp\")\n",
    "\n",
    "# fix for the meantime to ensure projections line up\n",
    "#coral_dat = coral_dat.to_crs('EPSG:3577')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'geopandas.geodataframe.GeoDataFrame'>\n",
      "    Latitude   Longitude  Algae  BMA  Branching  HCOther  Massive  Inverts  \\\n",
      "0 -23.914512  152.396780    2.0  0.0        2.0      0.0      8.0      2.0   \n",
      "1 -23.914497  152.396776    0.0  0.0        2.0      0.0     32.0      0.0   \n",
      "2 -23.914478  152.396769    0.0  0.0        8.0      0.0      8.0      0.0   \n",
      "3 -23.914444  152.396754    6.0  0.0        4.0      0.0     20.0      0.0   \n",
      "4 -23.914412  152.396745    4.0  0.0        4.0      4.0     10.0      0.0   \n",
      "\n",
      "   Plate  Rock  ...  Coral       DomBen  pa_branch  pa_massive pa_plate  \\\n",
      "0   20.0  20.0  ...   52.0  Coral/Algae          0           1        1   \n",
      "1    0.0   2.0  ...   74.0  Coral/Algae          0           1        0   \n",
      "2   12.0  14.0  ...   66.0  Coral/Algae          1           1        1   \n",
      "3    4.0   2.0  ...   78.0  Coral/Algae          0           1        0   \n",
      "4    0.0   0.0  ...   88.0  Coral/Algae          0           1        0   \n",
      "\n",
      "   pa_soft  dominante_  class_num  dom_num                     geometry  \n",
      "0        1       Mixed         15        6  POINT (152.39678 -23.91451)  \n",
      "1        1   SoftCoral         15        4  POINT (152.39678 -23.91450)  \n",
      "2        1   SoftCoral         15        4  POINT (152.39677 -23.91448)  \n",
      "3        1   SoftCoral         15        4  POINT (152.39675 -23.91444)  \n",
      "4        1   SoftCoral         15        4  POINT (152.39675 -23.91441)  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "130867\n",
      "epsg:4326\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# select required cols?\\ncoral_dat_lite = gpd.GeoDataFrame(coral_dat[[class_field]])\\ncoral_dat_lite['geometry'] = coral_dat.geometry\\nprint(coral_dat_lite.head())\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(coral_dat))\n",
    "print(coral_dat.head())\n",
    "print(coral_dat.shape[0])\n",
    "print(coral_dat.crs)\n",
    "\n",
    "# set the class field for sampling\n",
    "class_field = \"class_num\"\n",
    "\n",
    "'''\n",
    "# select required cols?\n",
    "coral_dat_lite = gpd.GeoDataFrame(coral_dat[[class_field]])\n",
    "coral_dat_lite['geometry'] = coral_dat.geometry\n",
    "print(coral_dat_lite.head())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(122, 24)\n"
     ]
    }
   ],
   "source": [
    "# just subsample and plot to check\n",
    "coral_dat['random'] = np.random.uniform(0, 1, coral_dat.shape[0])\n",
    "coral_dat_sample = coral_dat[coral_dat.random > 0.999]\n",
    "coral_dat_sample = coral_dat_sample.reset_index(drop=True)\n",
    "\n",
    "#coral_dat_buffs = coral_dat_sample\n",
    "#coral_dat_buffs['geometry'] = coral_dat_sample.geometry.buffer(0.0001)\n",
    "\n",
    "print(coral_dat_sample.shape)\n",
    "#print(coral_dat_buffs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_shapefile(coral_dat_sample, attribute = class_field)\n",
    "#map_shapefile(coral_dat_buffs, attribute = class_field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-image covariate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*still to come*\n",
    "- Depth\n",
    "- Wave climate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample model cal/val data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image data\n",
    "- Directly from imagery  \n",
    "- In long term this will be wrapped into a function that will generate the sample `time:` period dynamically based on the in situ data collection date\n",
    "- And when it get's big, there's be a flag to load the array of data and names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the dask cluster if it's running (try not to do any dask ops (e.g. geomedians) before this point)\n",
    "# HOW? the client object doesn't seem to be exported?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data already sampled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_image_sampling = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (run_image_sampling):\n",
    "    \n",
    "    samp_query = {\n",
    "        \"time\": (\"2019-06-01\", \"2019-06-30\"),\n",
    "        \"resolution\": (-10, 10),\n",
    "        'group_by' :'solar_day',\n",
    "        'measurements': ['nbar_coastal_aerosol', 'nbar_blue', 'nbar_green', 'nbar_red', 'nbar_nir_1']\n",
    "    }\n",
    "\n",
    "    column_names, model_input = collect_training_data(\n",
    "                                        gdf=coral_dat_sample, # use the subset while developing\n",
    "                                        products=[\"s2a_ard_granule\", \"s2b_ard_granule\"],\n",
    "                                        dc_query=samp_query,\n",
    "                                        ncpus=2,\n",
    "                                        custom_func=None,\n",
    "                                        field=class_field,\n",
    "                                        calc_indices=None,\n",
    "                                        reduce_func='median',\n",
    "                                        drop=False,\n",
    "                                        zonal_stats='mean')\n",
    "\n",
    "    # save the array (will eventually be big)\n",
    "    np.save('modeldat_names.npy', column_names, allow_pickle=False)\n",
    "    np.save('modeldat_input.npy', model_input, allow_pickle=False)\n",
    "    \n",
    "else:\n",
    "    column_names = np.load('modeldat_names.npy')\n",
    "    model_input = np.load('modeldat_input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['class_num' 'nbar_coastal_aerosol' 'nbar_blue' 'nbar_green' 'nbar_red'\n",
      " 'nbar_nir_1']\n",
      "[[  15. 1230.  822.  898.  301.   63.]\n",
      " [  15. 1168. 1424. 1611.  691.   93.]\n",
      " [  15. 1891. 1913. 2189. 1325.  623.]\n",
      " [  13.  828.  980. 1177.  508.  102.]\n",
      " [  11. 1623. 1882. 2404. 1595.  625.]]\n",
      "(113, 6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(column_names)\n",
    "print(model_input[0:5,])\n",
    "print(model_input.shape)\n",
    "column_names.__class__\n",
    "model_input.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack in/sample non-image data\n",
    "- depth\n",
    "- waves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "\n",
    "**Fit the model or load a fitted model??**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_fit = False\n",
    "model_filename = 'fitted_model.joblib'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into cal/val\n",
    "model_train, model_test = model_selection.train_test_split(model_input, \n",
    "                                                           stratify=model_input[:, 0],\n",
    "                                                           train_size=0.8, \n",
    "                                                           random_state=0)\n",
    "print(\"Train shape:\", model_train.shape)\n",
    "print(\"Test shape:\", model_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the variables we want to use to train our model\n",
    "model_variables = column_names[1:]\n",
    "\n",
    "# Extract relevant indices from the processed shapefile\n",
    "model_col_indices = [column_names.index(var_name) for var_name in model_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_fit:    \n",
    "    # Initialise random forest clasifier\n",
    "    model = RandomForestClassifier(n_estimators=100,\n",
    "                                   max_depth=20,\n",
    "                                   min_samples_split=2,\n",
    "                                   min_samples_leaf=2)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(model_train[:, model_col_indices], model_train[:, 0])\n",
    "    # save model\n",
    "    dump(model, model_filename)\n",
    "else:\n",
    "    model = load(model_filename)\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... skipping variable importance, as i haven't had much luck with it being useful! ...*\n",
    "\n",
    "Mainly interesting in accuracy since we have loads of data for validation\n",
    "\n",
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict into the test set and calcualte accuracy\n",
    "test_preds = model.predict(model_test[:, model_col_indices])\n",
    "accuracy_score(test_preds, model_test[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict model into environmental space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load covariate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image composite (temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the dask cluster\n",
    "create_local_dask_cluster()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test extent for now - this will eventually be controlled by appliction or just a whole extent run\n",
    "\n",
    "query = {\n",
    "    \"x\": (151.48837, 152.22467),\n",
    "    \"y\": (-23.17189, -23.60526),\n",
    "    \"time\": (\"2019-06-01\", \"2019-06-05\"),\n",
    "    \"group_by\": \"solar_day\",\n",
    "    \"resolution\": (-10, 10),\n",
    "    'measurements': ['nbar_coastal_aerosol', 'nbar_blue', 'nbar_green', 'nbar_red', 'nbar_nir_1'],\n",
    "    'output_crs': 'EPSG:3577'\n",
    "}\n",
    "\n",
    "s2_collection = load_ard(dc=dc,\n",
    "                         products=[\"s2a_ard_granule\", \"s2b_ard_granule\"],\n",
    "                         dask_chunks={\"time\": 1, \"x\": 2000, \"y\": 2000},\n",
    "                         dtype='native',\n",
    "                         **query)\n",
    "\n",
    "print(s2_collection)\n",
    "\n",
    "s2_geomedian = int_geomedian(s2_collection)\n",
    "# compute - ideally i'd like to be able to load the bands at this point - possible? e.g. s2_geomedian.some_bands.load()\n",
    "s2_geomedian = s2_geomedian.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s2_geomedian)\n",
    "rgb(s2_geomedian, bands = ['nbar_blue', 'nbar_green', 'nbar_red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack in non-image covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- depth\n",
    "- waves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction into image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the model into the imagery/covariates\n",
    "stack_prediction = predict_xr(model, s2_geomedian, progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stack_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot\n",
    "stack_prediction.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Plot classified image\n",
    "stack_prediction.plot(ax=axes[0],\n",
    "                      cmap='Set1', \n",
    "                      add_labels=False, \n",
    "                      add_colorbar=False)\n",
    "\n",
    "# Plot true colour image\n",
    "(s2_geomedian[['nbar_red', 'nbar_green', 'nbar_blue']]\n",
    " #.squeeze('time')\n",
    " .to_array()\n",
    " .plot.imshow(ax=axes[1], robust=True, add_labels=False))\n",
    "\n",
    "# Remove axis on right plot\n",
    "axes[1].get_yaxis().set_visible(False)\n",
    "\n",
    "# Add plot titles\n",
    "axes[0].set_title('Map prediction')\n",
    "axes[1].set_title('True colour image');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_cog(stack_prediction,\n",
    "         'cap_bunk_benthic.tif',\n",
    "          overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
